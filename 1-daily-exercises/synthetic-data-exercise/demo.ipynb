{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original columns from the dataset\n",
    "original_columns = data.columns.tolist()\n",
    " \n",
    "# New columns for the generated table structure\n",
    "generated_columns = ['ColumnNames', 'ColumnAcronyms', 'DataTypes', 'Nullability', 'ColumnDescriptions']\n",
    " \n",
    "# Initialize an empty dataframe to store the results\n",
    "# Note: We adjust the DataFrame initialization to better accommodate list storage for generated columns\n",
    "results_df = pd.DataFrame()\n",
    " \n",
    "results_df.head(10)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THE FIRST TIME YOU RUN THIS, IT MIGHT TAKE A WHILE\n",
    "# model_path_or_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path_or_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_path_or_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     # bnb_4bit_compute_dtype=torch.float16,\n",
    "#     use_flash_attention_2=True,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     # load_in_4bit=True,\n",
    "# ).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FIRST TIME YOU RUN THIS, IT MIGHT TAKE A WHILE\n",
    "model_path_or_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path_or_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    use_flash_attention_2=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table_structure(info):\n",
    "    \n",
    "    prompt = f\"Please generate a table of 5 column names, column acronym, data type, nullability, and column descriptions for the given information: {info}. In CSV format.\"\n",
    "    \"\"\"Convenience function for generating model output\"\"\"\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "# Task: Generate table rows based on the given column titles.\n",
    "# Column Titles: ColumnNames, ColumnAcronyms, DataTypes, Nullability, ColumnDescriptions\n",
    "# Output Format: Provide each row in CSV format.\n",
    "# Information for row: {info}\n",
    "#     \"\"\"\n",
    "    # Tokenize the input\n",
    "    input_ids = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True).input_ids.cuda()\n",
    "   \n",
    "    # Generate new tokens based on the prompt, up to max_new_tokens\n",
    "    # Sample aacording to the parameter\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=500,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.1,\n",
    "            use_cache=True\n",
    "        )\n",
    "    return tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
    " \n",
    "def parse_table_structure(table_text):\n",
    "    lines = table_text.split('\\n')\n",
    "    table_data = []\n",
    "    for line in lines:\n",
    "        fields = line.split(',')\n",
    "        # Adjust parsing to correctly create lists of lists for each generated row\n",
    "        if len(fields) >= len(generated_columns):  # Ensure we have the expected number of fields\n",
    "            table_data.append(fields[:len(generated_columns)])  # Append a list for each row\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "data = data.sample(frac=1).head(40)\n",
    "\n",
    "# Iterate through each row in the dataset and call the OpenAI API\n",
    "for index, row in data.iterrows():\n",
    "    count += 1\n",
    "    progress = count / len(data)*100\n",
    "    print(f'{progress:.2f}%')\n",
    "    generated_table_text = generate_table_structure(row.to_json())\n",
    "    print(generated_table_text)\n",
    "    \n",
    "    generated_table_data = parse_table_structure(generated_table_text)\n",
    " \n",
    "    # Each generated row is added to the results DataFrame\n",
    "    for generated_row in generated_table_data:\n",
    "        # Combine original row data with generated data\n",
    "        combined_row_data = row.tolist() + generated_row\n",
    "        # Dynamically adjust column names to accommodate both original and generated data\n",
    "        dynamic_columns = original_columns + [f'Generated_{col}' for col in generated_columns]\n",
    "        result_row = pd.DataFrame([combined_row_data], columns=dynamic_columns)\n",
    "        results_df = pd.concat([results_df, result_row], ignore_index=True)\n",
    " \n",
    "# Save the final dataframe to a CSV file\n",
    "output_file = 'data/generated_table_structures_with_original_data.csv'\n",
    "results_df.to_csv(output_file, index=False)\n",
    " \n",
    "print(f\"Data saved to {output_file}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table_structure(info):\n",
    "    prompt = f\"Please generate a table of 20 column names, column acronym, data type, nullability, and column descriptions for the given information: {info}. In CSV format.\"\n",
    "    \"\"\"Convenience function for generating model output\"\"\"\n",
    "    return prompt\n",
    "\n",
    "    #     # Tokenize the input\n",
    "    # input_ids = tokenizer(\n",
    "    #     prompt,\n",
    "    #     return_tensors=\"pt\",\n",
    "    #     truncation=True).input_ids.cuda()\n",
    "   \n",
    "    # # Generate new tokens based on the prompt, up to max_new_tokens\n",
    "    # # Sample aacording to the parameter\n",
    "    # with torch.inference_mode():\n",
    "    #     outputs = model.generate(\n",
    "    #         input_ids,\n",
    "    #         max_new_tokens=500,\n",
    "    #         do_sample=True,\n",
    "    #         top_p=0.9,\n",
    "    #         temperature=0.1,\n",
    "    #         use_cache=True\n",
    "    #     )\n",
    "    # return tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
    " \n",
    "def parse_table_structure(table_text):\n",
    "    lines = table_text.split('\\n')\n",
    "    table_data = []\n",
    "    for line in lines:\n",
    "        fields = line.split(',')\n",
    "        # Adjust parsing to correctly create lists of lists for each generated row\n",
    "        if len(fields) >= len(generated_columns):  # Ensure we have the expected number of fields\n",
    "            table_data.append(fields[:len(generated_columns)])  # Append a list for each row\n",
    "    return table_data\n",
    "\n",
    "\n",
    "count = 0\n",
    " \n",
    "create_prompt_list = []\n",
    "# Iterate through each row in the dataset and call the OpenAI API\n",
    "\n",
    "prompts = data.apply(lambda x: generate_table_structure(x.to_json()), axis=1)\n",
    "\n",
    "prompts = list(prompts.values)\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "model = LLM(\"mistralai/Mistral-7B-v0.1\")\n",
    "outputs = model.generate(prompts, sampling_params)\n",
    "\n",
    "tables = []\n",
    "for output in outputs:\n",
    "    count += 1\n",
    "\n",
    "    progress = count / len(data)*100\n",
    "    print(f'{progress:.2f}%')\n",
    "\n",
    "    tables.append(parse_table_structure(output.outputs[0].text))\n",
    " \n",
    "# Each generated row is added to the results DataFrame\n",
    "for generated_row in tables:\n",
    "    # Combine original row data with generated data\n",
    "    combined_row_data = row.tolist() + generated_row\n",
    "    # Dynamically adjust column names to accommodate both original and generated data\n",
    "    dynamic_columns = original_columns + [f'Generated_{col}' for col in generated_columns]\n",
    "    result_row = pd.DataFrame([combined_row_data], columns=dynamic_columns)\n",
    "    results_df = pd.concat([results_df, result_row], ignore_index=True)\n",
    " \n",
    "# Save the final dataframe to a CSV file\n",
    "output_file = 'data/generated_table_structures_with_original_data.csv'\n",
    "results_df.to_csv(output_file, index=False)\n",
    " \n",
    "print(f\"Data saved to {output_file}\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.11 (course-env)",
   "language": "python",
   "name": "course-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
