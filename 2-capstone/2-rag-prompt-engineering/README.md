# README for Retrieve-and-Generate (RAG) Pipeline

This README provides an overview of the Retrieve-and-Generate (RAG) pipeline implemented in the script `rag_pipeline.py`. This script and the accompanying Jupyter notebook guide users through the process of setting up and utilizing a RAG system. The RAG approach integrates document retrieval and language model generation to answer queries using context retrieved from a document database. This combination enhances the quality and relevance of the answers provided by the language model.

## Pipeline Overview

The RAG pipeline comprises several key components and steps:

1. **Document Retrieval**: Retrieves relevant document chunks from a vector database using a similarity search.
2. **Question Processing**: Takes a user-input question and processes it for the language model.
3. **Contextual Answer Generation**: Utilizes a large language model to generate answers based on the context provided by retrieved documents.
4. **Output Formatting**: Formats the generated answers for presentation to the user.

## Detailed Steps

### Step 1: Document Retrieval

This step involves setting up a connection to a PGVector database, where documents are stored as vector embeddings. The pipeline uses this connection to perform a similarity search to retrieve the top relevant documents based on the user's query. More about PGVector can be found in the [Langchain PGVector documentation](https://python.langchain.com/docs/integrations/vectorstores/pgvector/).

### Step 2: Question Processing

The user's query is processed to ensure it is in the correct format for the language model. This involves parsing and potentially modifying the query to fit the expected input structure of the model.

### Step 3: Contextual Answer Generation

The retrieved documents serve as context for a pre-trained transformer model, which generates an answer to the query. This model, [Mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1), is optimized for text generation based on provided context.

### Step 4: Output Formatting

The answers generated by the language model are formatted and presented to the user. This step ensures that the output is readable and effectively communicates the answer.

## Usage

To use this pipeline, follow these steps:
1. Ensure all dependencies, including `transformers`, `torch`, `langchain`, and `pgvector`, are installed.
2. Configure the script or notebook with the appropriate database connection parameters and model settings.
3. Execute the script or step through the notebook to process queries and generate answers.

## Script and Command-Line Interface

The script `rag_pipeline.py` is designed to be run from the command line. Here is how to use it:

```bash
usage: rag_pipeline.py [-h] --query QUERY [--generation_llm GENERATION_LLM] [--top_k TOP_K]

options:
  -h, --help            show this help message and exit
  --query QUERY         the query to process
  --generation_llm GENERATION_LLM
                        pretrained model ID to use for generation, default is 'mistralai/Mistral-7B-Instruct-v0.1'
  --top_k TOP_K         number of documents to retrieve for the RAG prompt, default is 10
```

## Conclusion

The RAG pipeline provides an advanced mechanism for generating contextually relevant answers by integrating retrieval and generation capabilities. This framework is suitable for educational purposes to demonstrate advanced NLP techniques and for real-world applications where accurate information retrieval is crucial. For a deeper understanding of the embedding mechanism used, see the [Sentence-BERT paper](https://arxiv.org/abs/1908.10084), "Sentence Embeddings using Siamese BERT-Networks" and the concept of [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity). The [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) model used for embeddings can be explored further on Hugging Face.