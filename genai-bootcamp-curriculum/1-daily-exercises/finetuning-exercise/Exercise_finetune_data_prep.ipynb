{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Data Preperation fo Instruction Fine Tuning** \n",
    "\n",
    "Today we will take the data set for our fake company using the synthetic data which has been cleansed as \"data/fixed_generated_table_structures_with_original_data.csv\"\n",
    "and format it into instruction, input, output pairs, save it to 'output.csv'. The next step we will format it so it is in an understandable format for fine tuning.\n",
    "\n",
    "We will be using the Alpaca format which is a model fine tuned on a LLama-2 model. You can find different formats fro different models online\n",
    "\n",
    "Load the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.11/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in ./venv/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./venv/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.11/site-packages (from scikit-learn) (3.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the cleansed synthetic data to be formatted and name the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the input and output file paths\n",
    "input_file = 'data/fixed_generated_table_structures_with_original_data.csv'\n",
    "output_file = 'output.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an instruction prompt, this can be anything you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the instruction text\n",
    "instruction = \"You are a column description generator. Given the following examples of Source System Name, Source System Acronym, Table Name, Table Description, ColumnNames, ColumnAcronyms, DataTypes, Nullability predict the Column Description \\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the csv file and read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open the input CSV file and read its contents\n",
    "with open(input_file, 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    data = list(reader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now combine the dataset into three columns Instruction, Input and the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reformatting complete. Output saved to output.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Open the output CSV file and write the reformatted data\n",
    "with open(output_file, 'w', newline='') as file:\n",
    "    fieldnames = ['Instruction', 'Input', 'Output']\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in data:\n",
    "        input_data = f\"{row['Source System Name']},{row['Source System Acronym']},{row['Table Name']},{row['Table Description']},{row['ColumnNames']},{row['ColumnAcronyms']},{row['DataTypes']},{row['Nullability']}\"\n",
    "        output_data = row['ColumnDescription']\n",
    "\n",
    "        writer.writerow({\n",
    "            'Instruction': instruction,\n",
    "            'Input': input_data,\n",
    "            'Output': output_data\n",
    "        })\n",
    "\n",
    "print(\"Reformatting complete. Output saved to\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the resulting file once verified its correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"output.csv\")\n",
    "df = df.fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we format this into \n",
    "\n",
    "**'### Instruction'**\n",
    "\n",
    "**'### Input'**\n",
    "\n",
    "**'### Response'**\n",
    "\n",
    "\n",
    "strings (which is the format used to create Alpaca (Stanford early model)). \n",
    "\n",
    "In the case where there is no input the prompt contains only an instruction and responce only format as Instruction + Response\n",
    "\n",
    "**'### Instruction'**\n",
    "\n",
    "**'### Response'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_col = []\n",
    "for _, row in df.iterrows():\n",
    "    prompt = \"Below is an instruction that describes a task, paired with are task that provides further context. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    instruction = str(row[\"Instruction\"])\n",
    "    input_query = str(row[\"Input\"])\n",
    "    response = str(row[\"Output\"])\n",
    "\n",
    "    if len(input_query.strip()) == 0:\n",
    "        text = prompt + \"### Instruction:\\n\" + instruction + \"\\n### Response:\\n\" + response\n",
    "    else:\n",
    "        text = (\n",
    "            prompt\n",
    "            + \"### Instruction:\\n\"\n",
    "            + instruction\n",
    "            + \"\\n### Input:\\n\"\n",
    "            + input_query\n",
    "            + \"\\n### Response:\\n\"\n",
    "            + response\n",
    "        )\n",
    "\n",
    "    text_col.append(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line df.loc[:, \"text\"] = text_col is assigning the values from the text_col list to a new column named \"text\" in the DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Instruction  \\\n",
      "0  You are a column description generator. Given ...   \n",
      "1  You are a column description generator. Given ...   \n",
      "2  You are a column description generator. Given ...   \n",
      "3  You are a column description generator. Given ...   \n",
      "4  You are a column description generator. Given ...   \n",
      "\n",
      "                                               Input  \\\n",
      "0  Exploration Data System,EDS,SeismicSurveyData_...   \n",
      "1  Exploration Data System,EDS,SeismicSurveyData_...   \n",
      "2  Exploration Data System,EDS,SeismicSurveyData_...   \n",
      "3  Exploration Data System,EDS,SeismicSurveyData_...   \n",
      "4  Exploration Data System,EDS,SeismicSurveyData_...   \n",
      "\n",
      "                                      Output  \\\n",
      "0  Unique identifier for each seismic survey   \n",
      "1                 Date of the seismic survey   \n",
      "2                                              \n",
      "3                                              \n",
      "4    Depth of the seismic survey measurement   \n",
      "\n",
      "                                                text  \n",
      "0  Below is an instruction that describes a task,...  \n",
      "1  Below is an instruction that describes a task,...  \n",
      "2  Below is an instruction that describes a task,...  \n",
      "3  Below is an instruction that describes a task,...  \n",
      "4  Below is an instruction that describes a task,...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.loc[:, \"text\"] = text_col\n",
    "print(df.head())\n",
    "df.to_csv(\"train.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the entire dataset df into training and temporary sets using train_test_split(df, test_size=0.3, random_state=42). This assigns 70% of the data (2,730 examples) to the training set and 30% (1,170 examples) to the temporary set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the temporary set temp_df into validation and testing sets using train_test_split(temp_df, test_size=0.5, random_state=42). This assigns 50% of the temporary set (585 examples) to the validation set and the remaining 50% (585 examples) to the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With this 70-15-15 split, you'll have more data for training while still having a sufficient number of examples for validation and testing. This can help improve the model's performance and generalization, especially when dealing with a smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the split datasets to separate CSV files\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "val_df.to_csv(\"val.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally prepared the cleansed data into the format we require for fine tuning and we have split it into training, validation and testing sets, taking random samples for each."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
