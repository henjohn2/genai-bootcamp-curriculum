{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47d7376-5f2d-4e35-a422-37e727a149f8",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "This notebook covers a few of the foundational techniques for dimensionality reduction in traditional NLP approaches. English words sit inside an enormous space. Merriam-Webster records 470,000 words in their unabridged dictionary. The corpus of English words (the words used by the entire set of English speakers) at any given point in recent history is about 170,000 words. The average English speaker uses about 30,000 words in their personal vocabulary. And this only represents our starting dataset. We are actually introduces in documents, which are large collections of English words. Even with a minimal vocabularly, there are more grammatically correct english sentences than there are atoms in the universe.\n",
    "\n",
    "Figuring out how to encode that much information into a numerical representation consumable by a computer has been the work of AI and NLP researchers since the 1950â€™s. The techniques that dominated that space until the introduction of the word embedding are explored (lightly) in this notebook. The goal isn't to learn these technqiues, but to gain some understanding of the problem that vector embedding will solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95bfdcef-cede-466d-bdc6-60322f65d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc818db-5910-40d6-9f71-4cce3c6af0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''NLP has used several different techniques for dimensionality reduction. The two most common are bag of words and TI/IDF (which are closely related). We also use other techniques from impoveing the signal, like removing stop words (the, and, to, etc), tokenization, stemming/lemmatization (running -> run, construction/constructs -> construct). The goal of all of these techniques is to create a numerical representation of the words in our corpus for consumption by the computer.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9506bf-05de-4c6b-9d0d-27afbe2731e5",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "Bag of words is one of the oldest techniques. The basic approach is to convert a document into a the vector of counts of words. If a word appears once in your document, than that word will have a value of one in the vector. Note that this actually encodes two pieces of information, the index that the word is mapped to as well as the value.\n",
    "\n",
    "To get started we will take the sentence above and parse it using basic Python data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b45cafe-d7e0-461f-9894-c780597fc6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# First we split the sentence into unique words\n",
    "words = text.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329bbfa9-a11d-4001-b49f-62cfd3772fcf",
   "metadata": {},
   "source": [
    "Unfortunately, we already have a problem. Splitting a sentence into words is not a straightforward exercise. We used the direct approach above of splitting on spaces, but that doesn't adequately address punctuation, which you can see below. Additionally, we need to account for differences in captialization. Should \"NLP\" be capitalized or lowercase? There are no hard and fast answers here. Our strategy for parsing this sentence depends on our goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e8081b-d2ee-49b4-8781-40e7d53080b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP',\n",
       " 'has',\n",
       " 'used',\n",
       " 'several',\n",
       " 'different',\n",
       " 'techniques',\n",
       " 'for',\n",
       " 'dimensionality',\n",
       " 'reduction.',\n",
       " 'The',\n",
       " 'two',\n",
       " 'most',\n",
       " 'common',\n",
       " 'are',\n",
       " 'bag',\n",
       " 'of',\n",
       " 'words',\n",
       " 'and',\n",
       " 'TI/IDF',\n",
       " '(which',\n",
       " 'are',\n",
       " 'closely',\n",
       " 'related).',\n",
       " 'We',\n",
       " 'also',\n",
       " 'use',\n",
       " 'other',\n",
       " 'techniques',\n",
       " 'from',\n",
       " 'impoveing',\n",
       " 'the',\n",
       " 'signal,',\n",
       " 'like',\n",
       " 'removing',\n",
       " 'stop',\n",
       " 'words',\n",
       " '(the,',\n",
       " 'and,',\n",
       " 'to,',\n",
       " 'etc),',\n",
       " 'tokenization,',\n",
       " 'stemming/lemmatization',\n",
       " '(running',\n",
       " '->',\n",
       " 'run,',\n",
       " 'construction/constructs',\n",
       " '->',\n",
       " 'construct).',\n",
       " 'The',\n",
       " 'goal',\n",
       " 'of',\n",
       " 'all',\n",
       " 'of',\n",
       " 'these',\n",
       " 'techniques',\n",
       " 'is',\n",
       " 'to',\n",
       " 'create',\n",
       " 'a',\n",
       " 'numerical',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'words',\n",
       " 'in',\n",
       " 'our',\n",
       " 'corpus',\n",
       " 'for',\n",
       " 'consumption',\n",
       " 'by',\n",
       " 'the',\n",
       " 'computer.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba7e125-99a0-49e0-8da9-19633b70716f",
   "metadata": {},
   "source": [
    "Our goal is to demonstrate the process so we can appreciate how great vector embeddings are; consequently, we'll take the straight forward approach and remove all punctuation and convert everything to lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4640b4b-d84d-4171-9f70-eba5908a76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to remove punctuation\n",
    "words_clean = [re.sub(r\"[^\\w\\s]\", \"\", word) for word in words]\n",
    "words_clean = [x.lower() for x in words_clean if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f64fb3-e296-4a5f-8e5b-d8a0ae8ccb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp',\n",
       " 'has',\n",
       " 'used',\n",
       " 'several',\n",
       " 'different',\n",
       " 'techniques',\n",
       " 'for',\n",
       " 'dimensionality',\n",
       " 'reduction',\n",
       " 'the',\n",
       " 'two',\n",
       " 'most',\n",
       " 'common',\n",
       " 'are',\n",
       " 'bag',\n",
       " 'of',\n",
       " 'words',\n",
       " 'and',\n",
       " 'tiidf',\n",
       " 'which',\n",
       " 'are',\n",
       " 'closely',\n",
       " 'related',\n",
       " 'we',\n",
       " 'also',\n",
       " 'use',\n",
       " 'other',\n",
       " 'techniques',\n",
       " 'from',\n",
       " 'impoveing',\n",
       " 'the',\n",
       " 'signal',\n",
       " 'like',\n",
       " 'removing',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'the',\n",
       " 'and',\n",
       " 'to',\n",
       " 'etc',\n",
       " 'tokenization',\n",
       " 'stemminglemmatization',\n",
       " 'running',\n",
       " 'run',\n",
       " 'constructionconstructs',\n",
       " 'construct',\n",
       " 'the',\n",
       " 'goal',\n",
       " 'of',\n",
       " 'all',\n",
       " 'of',\n",
       " 'these',\n",
       " 'techniques',\n",
       " 'is',\n",
       " 'to',\n",
       " 'create',\n",
       " 'a',\n",
       " 'numerical',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'words',\n",
       " 'in',\n",
       " 'our',\n",
       " 'corpus',\n",
       " 'for',\n",
       " 'consumption',\n",
       " 'by',\n",
       " 'the',\n",
       " 'computer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919d90c-bc0b-46a4-97a0-293132741643",
   "metadata": {},
   "source": [
    "To make the Bag of Words vector, we will use the default dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d91a3f-d0a9-43d8-9553-031526746fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ad3a0c9-9803-4272-8e5f-2c4550a4565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = defaultdict(int)\n",
    "for word in words_clean:\n",
    "    bag_of_words[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3907dad-3f3f-4317-9cae-f11a92fcf44e",
   "metadata": {},
   "source": [
    "Looking at the output, we can see that the count is dominated by words of super low informational value like \"the\" and \"of\". This are known as stop words and are frequently removed when utilizing an encoding like Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92c783ab-b5e9-4819-b7c5-299ebd2b8453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['nlp', '1'],\n",
       "       ['signal', '1'],\n",
       "       ['like', '1'],\n",
       "       ['removing', '1'],\n",
       "       ['stop', '1'],\n",
       "       ['etc', '1'],\n",
       "       ['tokenization', '1'],\n",
       "       ['stemminglemmatization', '1'],\n",
       "       ['running', '1'],\n",
       "       ['run', '1'],\n",
       "       ['constructionconstructs', '1'],\n",
       "       ['construct', '1'],\n",
       "       ['goal', '1'],\n",
       "       ['all', '1'],\n",
       "       ['these', '1'],\n",
       "       ['is', '1'],\n",
       "       ['create', '1'],\n",
       "       ['a', '1'],\n",
       "       ['numerical', '1'],\n",
       "       ['representation', '1'],\n",
       "       ['in', '1'],\n",
       "       ['our', '1'],\n",
       "       ['corpus', '1'],\n",
       "       ['consumption', '1'],\n",
       "       ['impoveing', '1'],\n",
       "       ['by', '1'],\n",
       "       ['from', '1'],\n",
       "       ['use', '1'],\n",
       "       ['has', '1'],\n",
       "       ['used', '1'],\n",
       "       ['several', '1'],\n",
       "       ['different', '1'],\n",
       "       ['dimensionality', '1'],\n",
       "       ['reduction', '1'],\n",
       "       ['two', '1'],\n",
       "       ['most', '1'],\n",
       "       ['other', '1'],\n",
       "       ['bag', '1'],\n",
       "       ['common', '1'],\n",
       "       ['tiidf', '1'],\n",
       "       ['which', '1'],\n",
       "       ['closely', '1'],\n",
       "       ['related', '1'],\n",
       "       ['we', '1'],\n",
       "       ['also', '1'],\n",
       "       ['computer', '1'],\n",
       "       ['are', '2'],\n",
       "       ['and', '2'],\n",
       "       ['to', '2'],\n",
       "       ['for', '2'],\n",
       "       ['words', '3'],\n",
       "       ['techniques', '3'],\n",
       "       ['of', '4'],\n",
       "       ['the', '6']], dtype='<U22')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[k, v] for k, v in bag_of_words.items()])\n",
    "a[a[:, 1].argsort()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c3e93-6c67-4919-84c9-b4941f047c1e",
   "metadata": {},
   "source": [
    "To remove the stop words I am going to use one of the Python NLP libraries. There is a lot of great functionality to be found in sPacy, but for our purposes we will initialize a simple NLP pipeline (the pipeline handles the myriad of parsing activities), process the text, then use the richly annotated output to identify stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "475c42f6-431e-43e8-8d09-d9a563d3b377",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a68383db-f1ca-4a16-b59a-e8b203232de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b24216eb-0d0c-471c-9430-a9c17d485744",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Token object:\n",
      "\n",
      "class Token(builtins.object)\n",
      " |  An individual token â€“ i.e. a word, punctuation symbol, whitespace,\n",
      " |  etc.\n",
      " |  \n",
      " |  DOCS: https://spacy.io/api/token\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bytes__(...)\n",
      " |      Token.__bytes__(self)\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(...)\n",
      " |      The number of unicode characters in the token, i.e. `token.text`.\n",
      " |      \n",
      " |      RETURNS (int): The number of unicode characters in the token.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#len\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Token.__reduce__(self)\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __unicode__(...)\n",
      " |      Token.__unicode__(self)\n",
      " |  \n",
      " |  check_flag(...)\n",
      " |      Token.check_flag(self, attr_id_t flag_id) -> bool\n",
      " |      Check the value of a boolean flag.\n",
      " |      \n",
      " |              flag_id (int): The ID of the flag attribute.\n",
      " |              RETURNS (bool): Whether the flag is set.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/token#check_flag\n",
      " |  \n",
      " |  has_dep(...)\n",
      " |      Token.has_dep(self)\n",
      " |      Check whether the token has annotated dep information.\n",
      " |              Returns False when the dep label is unset/missing.\n",
      " |      \n",
      " |              RETURNS (bool): Whether the dep label is valid or not.\n",
      " |  \n",
      " |  has_head(...)\n",
      " |      Token.has_head(self)\n",
      " |      Check whether the token has annotated head information.\n",
      " |              Return False when the head annotation is unset/missing.\n",
      " |      \n",
      " |              RETURNS (bool): Whether the head annotation is valid or not.\n",
      " |  \n",
      " |  has_morph(...)\n",
      " |      Token.has_morph(self)\n",
      " |      Check whether the token has annotated morph information.\n",
      " |              Return False when the morph annotation is unset/missing.\n",
      " |      \n",
      " |              RETURNS (bool): Whether the morph annotation is set.\n",
      " |  \n",
      " |  is_ancestor(...)\n",
      " |      Token.is_ancestor(self, descendant)\n",
      " |      Check whether this token is a parent, grandparent, etc. of another\n",
      " |              in the dependency tree.\n",
      " |      \n",
      " |              descendant (Token): Another token.\n",
      " |              RETURNS (bool): Whether this token is the ancestor of the descendant.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/token#is_ancestor\n",
      " |  \n",
      " |  nbor(...)\n",
      " |      Token.nbor(self, int i=1)\n",
      " |      Get a neighboring token.\n",
      " |      \n",
      " |              i (int): The relative position of the token to get. Defaults to 1.\n",
      " |              RETURNS (Token): The token at position `self.doc[self.i+i]`.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/token#nbor\n",
      " |  \n",
      " |  set_morph(...)\n",
      " |      Token.set_morph(self, features)\n",
      " |  \n",
      " |  similarity(...)\n",
      " |      Token.similarity(self, other)\n",
      " |      Make a semantic similarity estimate. The default estimate is cosine\n",
      " |              similarity using an average of word vectors.\n",
      " |      \n",
      " |              other (object): The object to compare with. By default, accepts `Doc`,\n",
      " |                  `Span`, `Token` and `Lexeme` objects.\n",
      " |              RETURNS (float): A scalar similarity score. Higher is more similar.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/token#similarity\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  get_extension(...) from builtins.type\n",
      " |      Token.get_extension(type cls, name)\n",
      " |      Look up a previously registered extension by name.\n",
      " |      \n",
      " |              name (str): Name of the extension.\n",
      " |              RETURNS (tuple): A `(default, method, getter, setter)` tuple.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/token#get_extension\n",
      " |  \n",
      " |  has_extension(...) from builtins.type\n",
      " |      Token.has_extension(type cls, name)\n",
      " |      Check whether an extension has been registered.\n",
      " |      \n",
      " |              name (str): Name of the extension.\n",
      " |              RETURNS (bool): Whether the extension has been registered.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/token#has_extension\n",
      " |  \n",
      " |  iob_strings(...) from builtins.type\n",
      " |      Token.iob_strings(type cls)\n",
      " |  \n",
      " |  remove_extension(...) from builtins.type\n",
      " |      Token.remove_extension(type cls, name)\n",
      " |      Remove a previously registered extension.\n",
      " |      \n",
      " |              name (str): Name of the extension.\n",
      " |              RETURNS (tuple): A `(default, method, getter, setter)` tuple of the\n",
      " |                  removed extension.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/token#remove_extension\n",
      " |  \n",
      " |  set_extension(...) from builtins.type\n",
      " |      Token.set_extension(type cls, name, **kwargs)\n",
      " |      Define a custom attribute which becomes available as `Token._`.\n",
      " |      \n",
      " |              name (str): Name of the attribute to set.\n",
      " |              default: Optional default value of the attribute.\n",
      " |              getter (callable): Optional getter function.\n",
      " |              setter (callable): Optional setter function.\n",
      " |              method (callable): Optional method for method extension.\n",
      " |              force (bool): Force overwriting existing attribute.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/token#set_extension\n",
      " |              USAGE: https://spacy.io/usage/processing-pipelines#custom-components-attributes\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  ancestors\n",
      " |      A sequence of this token's syntactic ancestors.\n",
      " |      \n",
      " |      YIELDS (Token): A sequence of ancestor tokens such that\n",
      " |          `ancestor.is_ancestor(self)`.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#ancestors\n",
      " |  \n",
      " |  children\n",
      " |      A sequence of the token's immediate syntactic children.\n",
      " |      \n",
      " |      YIELDS (Token): A child token such that `child.head==self`.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#children\n",
      " |  \n",
      " |  cluster\n",
      " |      RETURNS (int): Brown cluster ID.\n",
      " |  \n",
      " |  conjuncts\n",
      " |      A sequence of coordinated tokens, including the token itself.\n",
      " |      \n",
      " |      RETURNS (tuple): The coordinated tokens.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#conjuncts\n",
      " |  \n",
      " |  dep\n",
      " |      RETURNS (uint64): ID of syntactic dependency label.\n",
      " |  \n",
      " |  dep_\n",
      " |      RETURNS (str): The syntactic dependency label.\n",
      " |  \n",
      " |  doc\n",
      " |  \n",
      " |  ent_id\n",
      " |      RETURNS (uint64): ID of the entity the token is an instance of,\n",
      " |      if any.\n",
      " |  \n",
      " |  ent_id_\n",
      " |      RETURNS (str): ID of the entity the token is an instance of,\n",
      " |      if any.\n",
      " |  \n",
      " |  ent_iob\n",
      " |      IOB code of named entity tag. `1=\"I\", 2=\"O\", 3=\"B\"`. 0 means no tag\n",
      " |      is assigned.\n",
      " |      \n",
      " |      RETURNS (uint64): IOB code of named entity tag.\n",
      " |  \n",
      " |  ent_iob_\n",
      " |      IOB code of named entity tag. \"B\" means the token begins an entity,\n",
      " |      \"I\" means it is inside an entity, \"O\" means it is outside an entity,\n",
      " |      and \"\" means no entity tag is set. \"B\" with an empty ent_type\n",
      " |      means that the token is blocked from further processing by NER.\n",
      " |      \n",
      " |      RETURNS (str): IOB code of named entity tag.\n",
      " |  \n",
      " |  ent_kb_id\n",
      " |      RETURNS (uint64): Named entity KB ID.\n",
      " |  \n",
      " |  ent_kb_id_\n",
      " |      RETURNS (str): Named entity KB ID.\n",
      " |  \n",
      " |  ent_type\n",
      " |      RETURNS (uint64): Named entity type.\n",
      " |  \n",
      " |  ent_type_\n",
      " |      RETURNS (str): Named entity type.\n",
      " |  \n",
      " |  has_vector\n",
      " |      A boolean value indicating whether a word vector is associated with\n",
      " |      the object.\n",
      " |      \n",
      " |      RETURNS (bool): Whether a word vector is associated with the object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#has_vector\n",
      " |  \n",
      " |  head\n",
      " |      The syntactic parent, or \"governor\", of this token.\n",
      " |      If token.has_head() is `False`, this method will return itself.\n",
      " |      \n",
      " |      RETURNS (Token): The token predicted by the parser to be the head of\n",
      " |          the current token.\n",
      " |  \n",
      " |  i\n",
      " |  \n",
      " |  idx\n",
      " |      RETURNS (int): The character offset of the token within the parent\n",
      " |      document.\n",
      " |  \n",
      " |  is_alpha\n",
      " |      RETURNS (bool): Whether the token consists of alpha characters.\n",
      " |      Equivalent to `token.text.isalpha()`.\n",
      " |  \n",
      " |  is_ascii\n",
      " |      RETURNS (bool): Whether the token consists of ASCII characters.\n",
      " |      Equivalent to `[any(ord(c) >= 128 for c in token.text)]`.\n",
      " |  \n",
      " |  is_bracket\n",
      " |      RETURNS (bool): Whether the token is a bracket.\n",
      " |  \n",
      " |  is_currency\n",
      " |      RETURNS (bool): Whether the token is a currency symbol.\n",
      " |  \n",
      " |  is_digit\n",
      " |      RETURNS (bool): Whether the token consists of digits. Equivalent to\n",
      " |      `token.text.isdigit()`.\n",
      " |  \n",
      " |  is_left_punct\n",
      " |      RETURNS (bool): Whether the token is a left punctuation mark.\n",
      " |  \n",
      " |  is_lower\n",
      " |      RETURNS (bool): Whether the token is in lowercase. Equivalent to\n",
      " |      `token.text.islower()`.\n",
      " |  \n",
      " |  is_oov\n",
      " |      RETURNS (bool): Whether the token is out-of-vocabulary.\n",
      " |  \n",
      " |  is_punct\n",
      " |      RETURNS (bool): Whether the token is punctuation.\n",
      " |  \n",
      " |  is_quote\n",
      " |      RETURNS (bool): Whether the token is a quotation mark.\n",
      " |  \n",
      " |  is_right_punct\n",
      " |      RETURNS (bool): Whether the token is a right punctuation mark.\n",
      " |  \n",
      " |  is_sent_end\n",
      " |      A boolean value indicating whether the token ends a sentence.\n",
      " |      `None` if unknown. Defaults to `True` for the last token in the `Doc`.\n",
      " |      \n",
      " |      RETURNS (bool / None): Whether the token ends a sentence.\n",
      " |          None if unknown.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#is_sent_end\n",
      " |  \n",
      " |  is_sent_start\n",
      " |      A boolean value indicating whether the token starts a sentence.\n",
      " |      `None` if unknown. Defaults to `True` for the first token in the `Doc`.\n",
      " |      \n",
      " |      RETURNS (bool / None): Whether the token starts a sentence.\n",
      " |          None if unknown.\n",
      " |  \n",
      " |  is_space\n",
      " |      RETURNS (bool): Whether the token consists of whitespace characters.\n",
      " |      Equivalent to `token.text.isspace()`.\n",
      " |  \n",
      " |  is_stop\n",
      " |      RETURNS (bool): Whether the token is a stop word, i.e. part of a\n",
      " |      \"stop list\" defined by the language data.\n",
      " |  \n",
      " |  is_title\n",
      " |      RETURNS (bool): Whether the token is in titlecase. Equivalent to\n",
      " |      `token.text.istitle()`.\n",
      " |  \n",
      " |  is_upper\n",
      " |      RETURNS (bool): Whether the token is in uppercase. Equivalent to\n",
      " |      `token.text.isupper()`\n",
      " |  \n",
      " |  lang\n",
      " |      RETURNS (uint64): ID of the language of the parent document's\n",
      " |      vocabulary.\n",
      " |  \n",
      " |  lang_\n",
      " |      RETURNS (str): Language of the parent document's vocabulary,\n",
      " |      e.g. 'en'.\n",
      " |  \n",
      " |  left_edge\n",
      " |      The leftmost token of this token's syntactic descendents.\n",
      " |      \n",
      " |      RETURNS (Token): The first token such that `self.is_ancestor(token)`.\n",
      " |  \n",
      " |  lefts\n",
      " |      The leftward immediate children of the word, in the syntactic\n",
      " |      dependency parse.\n",
      " |      \n",
      " |      YIELDS (Token): A left-child of the token.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#lefts\n",
      " |  \n",
      " |  lemma\n",
      " |      RETURNS (uint64): ID of the base form of the word, with no\n",
      " |      inflectional suffixes.\n",
      " |  \n",
      " |  lemma_\n",
      " |      RETURNS (str): The token lemma, i.e. the base form of the word,\n",
      " |      with no inflectional suffixes.\n",
      " |  \n",
      " |  lex\n",
      " |      RETURNS (Lexeme): The underlying lexeme.\n",
      " |  \n",
      " |  lex_id\n",
      " |      RETURNS (int): Sequential ID of the token's lexical type.\n",
      " |  \n",
      " |  like_email\n",
      " |      RETURNS (bool): Whether the token resembles an email address.\n",
      " |  \n",
      " |  like_num\n",
      " |      RETURNS (bool): Whether the token resembles a number, e.g. \"10.9\",\n",
      " |      \"10\", \"ten\", etc.\n",
      " |  \n",
      " |  like_url\n",
      " |      RETURNS (bool): Whether the token resembles a URL.\n",
      " |  \n",
      " |  lower\n",
      " |      RETURNS (uint64): ID of the lowercase token text.\n",
      " |  \n",
      " |  lower_\n",
      " |      RETURNS (str): The lowercase token text. Equivalent to\n",
      " |      `Token.text.lower()`.\n",
      " |  \n",
      " |  morph\n",
      " |  \n",
      " |  n_lefts\n",
      " |      The number of leftward immediate children of the word, in the\n",
      " |      syntactic dependency parse.\n",
      " |      \n",
      " |      RETURNS (int): The number of leftward immediate children of the\n",
      " |          word, in the syntactic dependency parse.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#n_lefts\n",
      " |  \n",
      " |  n_rights\n",
      " |      The number of rightward immediate children of the word, in the\n",
      " |      syntactic dependency parse.\n",
      " |      \n",
      " |      RETURNS (int): The number of rightward immediate children of the\n",
      " |          word, in the syntactic dependency parse.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#n_rights\n",
      " |  \n",
      " |  norm\n",
      " |      RETURNS (uint64): ID of the token's norm, i.e. a normalised form of\n",
      " |      the token text. Usually set in the language's tokenizer exceptions\n",
      " |      or norm exceptions.\n",
      " |  \n",
      " |  norm_\n",
      " |      RETURNS (str): The token's norm, i.e. a normalised form of the\n",
      " |      token text. Usually set in the language's tokenizer exceptions or\n",
      " |      norm exceptions.\n",
      " |  \n",
      " |  orth\n",
      " |      RETURNS (uint64): ID of the verbatim text content.\n",
      " |  \n",
      " |  orth_\n",
      " |      RETURNS (str): Verbatim text content (identical to\n",
      " |      `Token.text`). Exists mostly for consistency with the other\n",
      " |      attributes.\n",
      " |  \n",
      " |  pos\n",
      " |      RETURNS (uint64): ID of coarse-grained part-of-speech tag.\n",
      " |  \n",
      " |  pos_\n",
      " |      RETURNS (str): Coarse-grained part-of-speech tag.\n",
      " |  \n",
      " |  prefix\n",
      " |      RETURNS (uint64): ID of a length-N substring from the start of the\n",
      " |      token. Defaults to `N=1`.\n",
      " |  \n",
      " |  prefix_\n",
      " |      RETURNS (str): A length-N substring from the start of the token.\n",
      " |      Defaults to `N=1`.\n",
      " |  \n",
      " |  prob\n",
      " |      RETURNS (float): Smoothed log probability estimate of token type.\n",
      " |  \n",
      " |  rank\n",
      " |      RETURNS (int): Sequential ID of the token's lexical type, used to\n",
      " |      index into tables, e.g. for word vectors.\n",
      " |  \n",
      " |  right_edge\n",
      " |      The rightmost token of this token's syntactic descendents.\n",
      " |      \n",
      " |      RETURNS (Token): The last token such that `self.is_ancestor(token)`.\n",
      " |  \n",
      " |  rights\n",
      " |      The rightward immediate children of the word, in the syntactic\n",
      " |      dependency parse.\n",
      " |      \n",
      " |      YIELDS (Token): A right-child of the token.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#rights\n",
      " |  \n",
      " |  sent\n",
      " |      RETURNS (Span): The sentence span that the token is a part of.\n",
      " |  \n",
      " |  sent_start\n",
      " |  \n",
      " |  sentiment\n",
      " |      RETURNS (float): A scalar value indicating the positivity or\n",
      " |      negativity of the token.\n",
      " |  \n",
      " |  shape\n",
      " |      RETURNS (uint64): ID of the token's shape, a transform of the\n",
      " |      token's string, to show orthographic features (e.g. \"Xxxx\", \"dd\").\n",
      " |  \n",
      " |  shape_\n",
      " |      RETURNS (str): Transform of the token's string, to show\n",
      " |      orthographic features. For example, \"Xxxx\" or \"dd\".\n",
      " |  \n",
      " |  subtree\n",
      " |      A sequence containing the token and all the token's syntactic\n",
      " |      descendants.\n",
      " |      \n",
      " |      YIELDS (Token): A descendent token such that\n",
      " |          `self.is_ancestor(descendent) or token == self`.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#subtree\n",
      " |  \n",
      " |  suffix\n",
      " |      RETURNS (uint64): ID of a length-N substring from the end of the\n",
      " |      token. Defaults to `N=3`.\n",
      " |  \n",
      " |  suffix_\n",
      " |      RETURNS (str): A length-N substring from the end of the token.\n",
      " |      Defaults to `N=3`.\n",
      " |  \n",
      " |  tag\n",
      " |      RETURNS (uint64): ID of fine-grained part-of-speech tag.\n",
      " |  \n",
      " |  tag_\n",
      " |      RETURNS (str): Fine-grained part-of-speech tag.\n",
      " |  \n",
      " |  tensor\n",
      " |  \n",
      " |  text\n",
      " |      RETURNS (str): The original verbatim text of the token.\n",
      " |  \n",
      " |  text_with_ws\n",
      " |      RETURNS (str): The text content of the span (with trailing\n",
      " |      whitespace).\n",
      " |  \n",
      " |  vector\n",
      " |      A real-valued meaning representation.\n",
      " |      \n",
      " |      RETURNS (numpy.ndarray[ndim=1, dtype='float32']): A 1D numpy array\n",
      " |          representing the token's semantics.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#vector\n",
      " |  \n",
      " |  vector_norm\n",
      " |      The L2 norm of the token's vector representation.\n",
      " |      \n",
      " |      RETURNS (float): The L2 norm of the vector representation.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/token#vector_norm\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  whitespace_\n",
      " |      RETURNS (str): The trailing whitespace character, if present.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __pyx_vtable__ = <capsule object NULL>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a166f09-93b7-4bbc-ac57-8410d7df14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = defaultdict(int)\n",
    "for word in [x for x in b if not x.is_stop]:\n",
    "    bag_of_words[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1fecb4f8-84e0-4d56-bb5d-d43ebdf33642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['NLP', '1'],\n",
       "       ['stop', '1'],\n",
       "       ['etc', '1'],\n",
       "       ['tokenization', '1'],\n",
       "       ['stemming', '1'],\n",
       "       ['lemmatization', '1'],\n",
       "       ['running', '1'],\n",
       "       ['run', '1'],\n",
       "       ['construction', '1'],\n",
       "       ['constructs', '1'],\n",
       "       ['construct', '1'],\n",
       "       ['goal', '1'],\n",
       "       ['create', '1'],\n",
       "       ['numerical', '1'],\n",
       "       ['representation', '1'],\n",
       "       ['corpus', '1'],\n",
       "       ['remove', '1'],\n",
       "       ['consumption', '1'],\n",
       "       ['like', '1'],\n",
       "       ['computer', '1'],\n",
       "       ['impovee', '1'],\n",
       "       ['use', '1'],\n",
       "       ['different', '1'],\n",
       "       ['related', '1'],\n",
       "       ['closely', '1'],\n",
       "       ['dimensionality', '1'],\n",
       "       ['IDF', '1'],\n",
       "       ['reduction', '1'],\n",
       "       ['common', '1'],\n",
       "       ['signal', '1'],\n",
       "       ['TI', '1'],\n",
       "       ['bag', '1'],\n",
       "       ['>', '2'],\n",
       "       ['-', '2'],\n",
       "       ['/', '3'],\n",
       "       ['(', '3'],\n",
       "       ['technique', '3'],\n",
       "       [')', '3'],\n",
       "       ['word', '3'],\n",
       "       ['.', '4'],\n",
       "       [',', '7']], dtype='<U21')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[k, v] for k, v in bag_of_words.items()])\n",
    "a[a[:, 1].argsort()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf92d95-aa3e-4b89-992f-4631ae24d0d1",
   "metadata": {},
   "source": [
    "Note how the tokens now include a lot of punctuation. We could (and should) remove these tokens, but we are now hitting the point of diminishing returns. Vector embeddings will solve nearly all of our problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e8d19-fadb-462a-a97c-ce8183b5824b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
