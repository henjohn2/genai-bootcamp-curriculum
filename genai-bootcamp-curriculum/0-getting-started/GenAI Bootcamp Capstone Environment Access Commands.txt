Server Access

MAC

ssh -i GenAI-Bootcamp.pem ubuntu@<instance DNS name provided to you> -L 7860:localhost:7860 -L 7861:localhost:7861 -L 8888:localhost:8888


Windows 

Configure as shown in the pictures

1. Install latest putty software through Software Center if you don't have latest one.
2. Create putty session using <instance DNS name provided to you>, port # 22 and save it with your preferred name i.e., Training
3. Add ppk file to session's ssh/auth
4. Add session's Connection/Proxy - Proxy hostname - web proxy.merck.com and Port - 8080
5. Add session's ssh/Tunnels - 



After server access is ready

Part 1 - Installation

Login to instance

$ conda init

Log out of Instance and Log back in

$ conda activate pytorch

$ git clone https://github.com/gen-ai-trainings/ft-lab && cd ft-lab

$ docker compose up -d 

$ cd llm && pip install -r requirements.txt && bash install_flash_attention2.sh 


Part 2 - Keeping a Jupyter Lab Instance Alive in Background

$ tmux new -s jupyter

$ conda activate pytorch

$ jupyter lab

Copy and Remember the URL

To get out of tmux session

Press `Ctrl + b` then press `d`

To join the existing tmux session

$ tmux a -t jupyter


S3 commands

# Check if you can access s3 from your instance

$ aws s3 ls s3://merck-genai-lab/

# If you get aws cli not installed error,  you install it

sudo apt  install awscli

# Check again

aws s3 ls s3://merck-genai-lab/

# You can copy the files from s3 to your instance as shown 

aws s3 cp s3://merck-genai-lab/msl-data/MSL-CallNotes-.zip .


# Helpful Tips for Environments

# Be careful of kernels
# A kernel will stay alive even if you close the notebook, this could consume GPU VRAM

$ nvidia-smi #shows VRAM usage

# When previous notebook exercises holding memory. 

# In respective notebook, “Shut Down all Kernels”, “Shut Down Kernel”

You will not be able to fine-tune and test at the same time
Finetuning nearly maxes out the VRAM, very tight memory budget


Lab exercises flow

1. I_Vector_DB

2. II_RAG_Prompt_Engineering

3. III_Finetuning_For_RAG
 - III_Finetuning_For_RAG/Instruction Finetuning LLMs with QLoRA for RAG.ipynb

4. A_I_Inference_Methods/scripts/gradio_app_with_context.py

Go through readme docs of respective exercises



